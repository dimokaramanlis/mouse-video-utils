{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b4c7a0",
   "metadata": {},
   "source": [
    "# Run Deeplabut for freely moving box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e51955",
   "metadata": {},
   "source": [
    "We will first import the libraries needed. If you get an error, check that you're running the notebook inside the environment where DeepLabCut is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d7beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc8...\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "import os, glob, re, datetime, shutil\n",
    "import dlc_fun\n",
    "\n",
    "# we will set some general path identifiers here\n",
    "datapath       = 'S:\\\\ElboustaniLab\\\\#SHARE\\\\Data'\n",
    "jointmod       = '0Dyad_JointPerceptualDecisionMaking'\n",
    "ffmpegpath     = 'C:\\\\ffmpeg\\\\bin'\n",
    "filmidentifier = 'Filming'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010b145",
   "metadata": {},
   "source": [
    "We will now specify the model for analysis. Only one model is available, for unimplanted mice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe54ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_twomice = 'S:\\\\ElboustaniLab\\\\#SHARE\\\\Analysis\\\\JointDecisionDeepLabCut\\\\TwoMiceTorch-DK-2025-04-15'\n",
    "project_slider  = 'S:\\\\ElboustaniLab\\\\#SHARE\\\\Analysis\\\\JointDecisionDeepLabCut\\\\MouseSlider-DK-2025-04-15'\n",
    "config_twomice  = os.path.join(project_twomice,'config.yaml')\n",
    "config_slider   = os.path.join(project_slider,'config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d6f9b",
   "metadata": {},
   "source": [
    "### Find videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79263382",
   "metadata": {},
   "source": [
    "Specify the mouse or pair for which you want to perform the analysis. This string should match folder namings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d36ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mousefolders = ['YX018', 'YX019', 'YX020', 'YX021',  \n",
    "#                'Slider_YX015','Slider_YX014',\n",
    "#               'YX015', 'YX014', 'YX017', 'YX016']\n",
    "mousefolders = ['YX015', 'YX014', 'YX017', 'YX016']\n",
    "datestart    = '20220201'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4834eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. data path found: S:\\ElboustaniLab\\#SHARE\\Data\\0Dyad_JointPerceptualDecisionMaking\\Slider_YX014\n"
     ]
    }
   ],
   "source": [
    "mousepaths = []\n",
    "for imouse in range(len(mousefolders)):\n",
    "    if '_' in mousefolders[imouse]:\n",
    "        mousepaths.append(os.path.join(datapath, jointmod, mousefolders[imouse]))\n",
    "    else:\n",
    "        mousepaths.append(os.path.join(datapath, mousefolders[imouse]))\n",
    "        \n",
    "for impath in range(len(mousepaths)):\n",
    "    if os.path.isdir(mousepaths[impath]): \n",
    "        print(str(impath+1) + '. data path found: ' + mousepaths[impath])\n",
    "    else:\n",
    "        print(str(impath+1) + 'no such mouse or pair, check your list')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7e9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "listvidpaths = []\n",
    "for imfold in range(len(mousepaths)):\n",
    "    allfpaths = dlc_fun.find_mp4_files(mousepaths[imfold])\n",
    "    listvidpaths.extend(allfpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d5738f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files were found that have not been analyzed:\n",
      "S:\\ElboustaniLab\\#SHARE\\Data\\0Dyad_JointPerceptualDecisionMaking\\Slider_YX014\\Filming\\SliderSingleMouse\\20250411\\Session1\\20250411_1851_Slider_YX014_1_reduced.mp4\n",
      "S:\\ElboustaniLab\\#SHARE\\Data\\0Dyad_JointPerceptualDecisionMaking\\Slider_YX014\\Filming\\SliderSingleMouse\\20250410\\Session1\\20250410_1916_Slider_YX014_1_reduced.mp4\n",
      "S:\\ElboustaniLab\\#SHARE\\Data\\0Dyad_JointPerceptualDecisionMaking\\Slider_YX014\\Filming\\SliderSingleMouse\\20250409\\Session1\\20250409_1938_Slider_YX014_2_reduced.mp4\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples (date, path)\n",
    "dated_paths = [(dlc_fun.extract_date(path), path) for path in listvidpaths]\n",
    "\n",
    "# Filter out None dates and sort the list by date\n",
    "sorted_paths = sorted((dp for dp in dated_paths if dp[0] is not None), key=lambda x: x[0])\n",
    "\n",
    "datethres    = datetime.datetime.strptime(datestart, '%Y%m%d')\n",
    "\n",
    "# Extract the sorted paths\n",
    "sorted_paths_only = [path for date, path in sorted_paths \n",
    "                     if datetime.datetime.strptime(date, '%Y%m%d') >= datethres and \n",
    "                     ('Unsorted' not in path) and ('Habituation' not in path) and \n",
    "                     ('Observational' not in path) and  ('ToSort' not in path) and  ('Direction' not in path)]\n",
    " \n",
    "# paths analyzed\n",
    "pathsold = [path for path in sorted_paths_only if not dlc_fun.to_analyze_dlc(path)]\n",
    "\n",
    "# clean up pickle and h5 pathsold\n",
    "for ipath in range(len(pathsold)):\n",
    "    dlc_fun.remove_big_files(os.path.splitext(pathsold[ipath])[0])\n",
    "        \n",
    "# paths to run DLC for\n",
    "pathsrun = [path for path in sorted_paths_only if dlc_fun.to_analyze_dlc(path)]\n",
    "pathsrun.reverse()\n",
    "\n",
    "if (len(sorted_paths_only)==0):\n",
    "    print('No video folder found.')\n",
    "else:\n",
    "    print('The following files were found that have not been analyzed:')\n",
    "    for ivid in range(len(pathsrun)):\n",
    "        print(pathsrun[ivid])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e55d22-7de0-43fd-9c09-013546c6df7b",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc3b92-ed7e-4858-913a-e91536aa5a60",
   "metadata": {},
   "source": [
    "Let's run deeplabcut now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93956cb2-6450-4873-98f7-78bc9d2dd362",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file to C:\\Temp_proc\\20250411_1851_Slider_YX014_1_reduced.mp4\n",
      "cropping for slider, assering slider is on TOP...\n",
      "Analyzing videos with S:\\ElboustaniLab\\#SHARE\\Analysis\\JointDecisionDeepLabCut\\MouseSlider-DK-2025-04-15\\dlc-models-pytorch\\iteration-2\\MouseSliderApr15-trainset95shuffle1\\train\\snapshot-best-125.pt\n",
      "Starting to analyze C:\\Temp_proc\\20250411_1851_Slider_YX014_1_reduced.mp4\n",
      "Video metadata: \n",
      "  Overall # of frames:    478927\n",
      "  Duration of video [s]:  15964.23\n",
      "  fps:                    30.0\n",
      "  resolution:             w=1408, h=1197\n",
      "\n",
      "Running pose prediction with batch size 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 279/478927 [00:39<18:38:20,  7.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     cropval \u001b[38;5;241m=\u001b[39m dlc_fun\u001b[38;5;241m.\u001b[39mget_mouse_compartment(localpath, isslider)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# run DLC locally\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcropping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcropval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mauto_track\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mn_tracks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msave_as_csv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m csvpath \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(localpath)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*_el.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# if video was cropped, fix csv file\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\compat.py:954\u001b[0m, in \u001b[0;36manalyze_videos\u001b[1;34m(config, videos, videotype, shuffle, trainingsetindex, gputouse, save_as_csv, in_random_order, destfolder, batchsize, cropping, TFGPUinference, dynamic, modelprefix, robust_nframes, allow_growth, use_shelve, auto_track, n_tracks, animal_names, calibrate, identity_only, use_openvino, engine, **torch_kwargs)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m             torch_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batchsize\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m analyze_videos(\n\u001b[0;32m    955\u001b[0m         config,\n\u001b[0;32m    956\u001b[0m         videos\u001b[38;5;241m=\u001b[39mvideos,\n\u001b[0;32m    957\u001b[0m         videotype\u001b[38;5;241m=\u001b[39mvideotype,\n\u001b[0;32m    958\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    959\u001b[0m         trainingsetindex\u001b[38;5;241m=\u001b[39mtrainingsetindex,\n\u001b[0;32m    960\u001b[0m         save_as_csv\u001b[38;5;241m=\u001b[39msave_as_csv,\n\u001b[0;32m    961\u001b[0m         in_random_order\u001b[38;5;241m=\u001b[39min_random_order,\n\u001b[0;32m    962\u001b[0m         destfolder\u001b[38;5;241m=\u001b[39mdestfolder,\n\u001b[0;32m    963\u001b[0m         dynamic\u001b[38;5;241m=\u001b[39mdynamic,\n\u001b[0;32m    964\u001b[0m         modelprefix\u001b[38;5;241m=\u001b[39mmodelprefix,\n\u001b[0;32m    965\u001b[0m         use_shelve\u001b[38;5;241m=\u001b[39muse_shelve,\n\u001b[0;32m    966\u001b[0m         robust_nframes\u001b[38;5;241m=\u001b[39mrobust_nframes,\n\u001b[0;32m    967\u001b[0m         auto_track\u001b[38;5;241m=\u001b[39mauto_track,\n\u001b[0;32m    968\u001b[0m         n_tracks\u001b[38;5;241m=\u001b[39mn_tracks,\n\u001b[0;32m    969\u001b[0m         animal_names\u001b[38;5;241m=\u001b[39manimal_names,\n\u001b[0;32m    970\u001b[0m         calibrate\u001b[38;5;241m=\u001b[39mcalibrate,\n\u001b[0;32m    971\u001b[0m         identity_only\u001b[38;5;241m=\u001b[39midentity_only,\n\u001b[0;32m    972\u001b[0m         overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    973\u001b[0m         cropping\u001b[38;5;241m=\u001b[39mcropping,\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtorch_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function is not implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\apis\\videos.py:545\u001b[0m, in \u001b[0;36manalyze_videos\u001b[1;34m(config, videos, videotype, shuffle, trainingsetindex, save_as_csv, in_random_order, snapshot_index, detector_snapshot_index, device, destfolder, batch_size, detector_batch_size, dynamic, ctd_conditions, ctd_tracking, top_down_dynamic, modelprefix, use_shelve, robust_nframes, transform, auto_track, n_tracks, animal_names, calibrate, identity_only, overwrite, cropping, save_as_df)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    544\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m [time\u001b[38;5;241m.\u001b[39mtime()]\n\u001b[1;32m--> 545\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     runtime\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m    553\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _generate_metadata(\n\u001b[0;32m    554\u001b[0m         cfg\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mproject_cfg,\n\u001b[0;32m    555\u001b[0m         pytorch_config\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mmodel_cfg,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    562\u001b[0m         robust_nframes\u001b[38;5;241m=\u001b[39mrobust_nframes,\n\u001b[0;32m    563\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\apis\\videos.py:204\u001b[0m, in \u001b[0;36mvideo_inference\u001b[1;34m(video, pose_runner, detector_runner, cropping, shelf_writer, robust_nframes)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m--> 204\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\runners\\inference.py:142\u001b[0m, in \u001b[0;36mInferenceRunner.inference\u001b[1;34m(self, images, shelf_writer)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(data)\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_full_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_results(shelf_writer)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Process the last batch even if not full\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\runners\\inference.py:203\u001b[0m, in \u001b[0;36mInferenceRunner._process_full_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes prepared inputs in batches of the desired batch size.\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\runners\\inference.py:246\u001b[0m, in \u001b[0;36mInferenceRunner._process_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m    242\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    243\u001b[0m     mk: v[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size] \u001b[38;5;28;01mfor\u001b[39;00m mk, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    244\u001b[0m }\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# remove processed inputs from batch\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\runners\\inference.py:303\u001b[0m, in \u001b[0;36mPoseInferenceRunner.predict\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mcrop(inputs)\n\u001b[0;32m    302\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 303\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    307\u001b[0m         raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    308\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\models\\model.py:136\u001b[0m, in \u001b[0;36mPoseModel.get_predictions\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract method for the forward pass of the Predictor.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m        A dictionary containing the predictions of each head group\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    137\u001b[0m         name: head\u001b[38;5;241m.\u001b[39mpredictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strides[name], outputs[name])\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    139\u001b[0m     }\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_features:\n\u001b[0;32m    141\u001b[0m         predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\models\\model.py:137\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract method for the forward pass of the Predictor.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m        A dictionary containing the predictions of each head group\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 137\u001b[0m         name: \u001b[43mhead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strides\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    139\u001b[0m     }\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_features:\n\u001b[0;32m    141\u001b[0m         predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_pytorch\\models\\predictors\\paf_predictor.py:146\u001b[0m, in \u001b[0;36mPartAffinityFieldPredictor.forward\u001b[1;34m(self, stride, outputs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Filter predicted heatmaps with a 2D Gaussian kernel as in:\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf\u001b[39;00m\n\u001b[0;32m    143\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_2d_gaussian_kernel(\n\u001b[0;32m    144\u001b[0m     sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnms_radius \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    145\u001b[0m )[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 146\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheatmaps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m heatmaps \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    148\u001b[0m     heatmaps, kernel, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m, groups\u001b[38;5;241m=\u001b[39mn_channels\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    151\u001b[0m peaks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_local_peak_indices_maxpool_nms(\n\u001b[0;32m    152\u001b[0m     heatmaps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnms_radius, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m    153\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ipath in range(len(pathsrun)):\n",
    "    \n",
    "    currpath = pathsrun[ipath]\n",
    "    # find if we can crop video\n",
    "    isslider = 'Slider' in currpath\n",
    "    ispair   = (jointmod in currpath) and (not isslider)\n",
    "    skipcrop = ispair or ('Visual' in currpath)\n",
    "    \n",
    "    if isslider:\n",
    "        config = config_slider\n",
    "        nt     = 1\n",
    "    else:\n",
    "        config = config_twomice\n",
    "        nt     = 2\n",
    "    \n",
    "    if \"reduced\" in currpath.lower():\n",
    "        # video is already compressed by us\n",
    "        localpath = dlc_fun.copy_video_locally(currpath, 'C:\\\\Temp_proc')\n",
    "    else:\n",
    "        # we compress\n",
    "        localpath, status = dlc_fun.ffmpeg_compress_mp4_video(currpath, 'C:\\\\Temp_proc')\n",
    "                                                  \n",
    "    # do cropping\n",
    "    if skipcrop:\n",
    "        cropval = None\n",
    "    else:\n",
    "        cropval = dlc_fun.get_mouse_compartment(localpath, isslider)\n",
    "        \n",
    "    # run DLC locally\n",
    "    deeplabcut.analyze_videos(config, localpath, \n",
    "                              videotype = 'mp4', \n",
    "                              cropping = cropval,\n",
    "                              auto_track = True, \n",
    "                              n_tracks = nt, \n",
    "                              save_as_csv = True, \n",
    "                              batchsize = 4)\n",
    "    \n",
    "    csvpath = glob.glob(os.path.splitext(localpath)[0] + '*_el.csv')\n",
    "    \n",
    "    # if video was cropped, fix csv file\n",
    "    if not skipcrop:\n",
    "        dlc_fun.update_saved_csv(csvpath[0], cropval[2])\n",
    "\n",
    "    # move dlc csv back to path\n",
    "    shutil.copy2(csvpath[0], os.path.split(currpath)[0])\n",
    "    \n",
    "    # remove h5 and pickles after you're done, and other local files\n",
    "    dlc_fun.remove_big_files(os.path.splitext(localpath)[0])\n",
    "    os.remove(localpath)\n",
    "    os.remove(csvpath[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
